---
title: "DeepSeek 笔记"
date: 2025-02-03 08:00:00 +0800
published: true
mermaid: true
math: true
categories: [技术]
tags: [AI, transformer]
---

## DeepSeekMoE

分为 Shared Expert(常驻) 和 Router Expert(共享)

Router Expert 会被分为几组，通过 Router 进行负载均衡，将问题分配到合适的组中。这是为了使用多节点训练是避免通信消耗，一般一个节点存放一组专家，同一个问题放在一个节点上处理，避免了多节点间的通信。

多节点间的前向和反向传播（pipeline parallel 流水线并行）：

![alt text](/assets/img/2025-02-03-deepseek/v2-20bf8bdded50d48b114381783a43de76_b.webp)

> 数据就像流水线一样在不同节点间通过，每个节点处理完一个数据就可以立即处理下一个数据，就像工厂里的流水线一样，每道工序都饱和运转。

多节点间同 batch 反向传播时的梯度整合（Data parallel 数据并行）：

![alt text](/assets/img/2025-02-03-deepseek/parallel.png)

## Multi-Token Prediction

![alt text](/assets/img/2025-02-03-deepseek/image.png)

Token Prediction 的意图是通过当前 token 预测下一个(组) token，图中最左边输入为 $t_1$ 到 $t_4$ ，参考的结果为 $t_2$ 到 $t_5$，也就是用 $t_1$ 预测 $t_2$ ，以此类推。但仅有最左边部分无法做到让 $t_1$ 预测 $t_3$，换句话说，$t_1$ 无法为预测 $t_3$ 做出贡献。于是添加了后面几个层，这些层都是用前一个层的输出作为输入，所以 $t_1$ 的信息被不断传递，每个 Cross-Entropy Loss 都会被用于改进最左边的主模型的权重。

## 专家负载均衡

训练过程记录每个 Router Expert 的负载率，仅激活负载率较低且与问题较相关的专家。

## 强化学习

### Reward Model

使用 DeepSeek v3 的一个检查点开始训练专用的 Reward Model ，用来给主模型进行强化训练，奖励规则：

1. 代码、数学、agent等有明确过程和结果的任务，无需使用Reward Model
2. 自由形式，但有明确结果的任务，由


> agent 就是模拟人类解决现实问题的方式执行对应步骤解决问题，也就是说它也要能拆分问题、运用工具、查找资料、进行决策等

## 参考

- [【深度学习】【分布式训练】一文捋顺千亿模型训练技术：流水线并行、张量并行和 3D 并行](https://zhuanlan.zhihu.com/p/617087561)
- [Deepseek v3 技术报告万字硬核解读](https://zhuanlan.zhihu.com/p/16323685381)
